# -*- coding: utf-8 -*-
"""finalmllvadsusr194_varundaslab2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19r9yS7QWxubdD2GcoLpU4gK-f_4RhZGq
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.cluster.hierarchy import fcluster
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler,LabelEncoder
le=LabelEncoder()
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report, f1_score
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier
from imblearn.over_sampling import SMOTE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

import warnings
warnings.filterwarnings("ignore")

df=pd.read_csv("/content/penguins_classification.csv")
df.head()

df.shape

df.info()

df.describe(include="all")

df.columns

df.columns

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.duplicated().sum()

df.dtypes

"""**UNIVARIATE ANALYSIS**"""

for column in df.select_dtypes(include={'float64','int64'}).columns:
  plt.figure(figsize=(10,5))
  sns.histplot(df[column])
  plt.title(f'Histogram of {column}')
  plt.xlabel(column)
  plt.ylabel('Frequency')
  plt.show()

"""**BIVARIATE ANALYSIS**"""

numerical_columns=df.select_dtypes(include={'float64','int64'}).columns
numerical_columns

for i in range(len(numerical_columns)):
    for j in range(i + 1, len(numerical_columns)):
        plt.figure(figsize=(10, 6))
        sns.scatterplot(data=df, x=numerical_columns[i], y=numerical_columns[j])
        plt.title(f'Scatter Plot between {numerical_columns[i]} and {numerical_columns[j]}')
        plt.show()

#correlation matrix
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
correlation_matrix = df[numerical_columns].corr()
print("Correlation matrix:\n", correlation_matrix)

plt.figure(figsize=(14, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Heatmap of Correlation Matrix')
plt.show()

"""**NULL VALUE TREATMENT**"""

df.isnull().sum()

df.bill_depth_mm=df.bill_depth_mm.fillna(df.bill_depth_mm.median())

df.isnull().sum()

#Outlier boxplot
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
for column in numerical_columns:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=df[column])
    plt.title(f'Box Plot of {column}')
    plt.xlabel(column)
    plt.show()

#Outlier detection
# Calculate Q1, Q3, and IQR
q1 = np.quantile(df["bill_length_mm"] , 0.25)
q3 = np.quantile(df["bill_length_mm"] , 0.75)
iqr = q3 - q1

# Calculate outlier bounds
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

print("Q1:", q1)
print("Q3:", q3)
print("IQR:", iqr)
print("Lower Bound (Outlier):", lower_bound)
print("Upper Bound (Outlier):", upper_bound)

# Identify outliers
outliers = [i for i in df["bill_length_mm"] if i < lower_bound or i > upper_bound]
print("Outliers:", outliers)

#outlier treatment
df.loc[df['bill_length_mm']>46.6750,'thinness_1_19_years']=46.6750
df.loc[df['bill_length_mm']<38.3499,'thinness_1_19_years']=38.3499

df.dtypes

#data type conversion-label encoder
df.species=le.fit_transform(df.species)
df.island=le.fit_transform(df.island)

"""**Class Imbalance**"""

x = df.drop(columns = ['species','thinness_1_19_years'])
y = df[['species']]

x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2)

"""**KNN**"""

model =  KNeighborsClassifier(n_neighbors=2)
model.fit(x_train, y_train)
# Predictions
y_pred = model.predict(x_test)
# Evaluation
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Precision
precision = precision_score(y_test, y_pred)
print("Precision:", precision)

# Recall
recall = recall_score(y_test, y_pred)
print("Recall:", recall)

# F1-score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, y_pred)
print("F1-score:", f1_score)

"""**SVM**"""

model =  SVC()
model.fit(x_train, y_train)
# Predictions
y_pred = model.predict(x_test)
# Evaluation
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Precision
precision = precision_score(y_test, y_pred)
print("Precision:", precision)

# Recall
recall = recall_score(y_test, y_pred)
print("Recall:", recall)

# F1-score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, y_pred)
print("F1-score:", f1_score)