# -*- coding: utf-8 -*-
"""finalmllvadsusr194_varundaslab3mic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lw9qpczBS9iKB6GAxtsinlmEjgNw0_dH
"""

import numpy as np
import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
from sklearn.tree import export_graphviz
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
le=LabelEncoder()
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, IsolationForest
from sklearn.model_selection import cross_val_score
from xgboost import XGBClassifier
import xgboost
from lightgbm import LGBMClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_curve, roc_auc_score, precision_recall_curve, auc, r2_score, mean_squared_error, accuracy_score, recall_score, silhouette_score, silhouette_samples
warnings.filterwarnings('ignore')

df = pd.read_csv("/content/customer_segmentation.csv")

df.shape

df.head()

df.dtypes

df.isna().sum()

df.duplicated().sum()

df = df.dropna()

for col in df.select_dtypes(include = ['number']).columns:
  sns.histplot(df[[col]])
  plt.show()

for i in df.select_dtypes(include = ['number']).columns:
  df[i] = df[i].fillna(df[i].median())

plt.figure(figsize=(20,12))
sns.boxplot(df)

for i in df.select_dtypes(include='number').columns:
  q1=df[i].quantile(0.25)
  q3=df[i].quantile(0.75)
  iqr=q3-q1
  l=q1-1.5*iqr
  u=q3+1.5*iqr
  df[i]=df[i].clip(lower=l,upper=u)

df.dtypes

df.Education=le.fit_transform(df.Education)
df.Marital_Status=le.fit_transform(df.Marital_Status)
df.Dt_Customer=le.fit_transform(df.Dt_Customer)

sns.heatmap(df.corr(),annot=True)

scaler = MinMaxScaler()
scaled = scaler.fit_transform(df)

km = KMeans(n_clusters=3)
km.fit(scaled)
y_pred = km.labels_

df['cluster'] = y_pred

df1 = df[df.cluster == 0]
df2 = df[df.cluster == 1]
df3 = df[df.cluster == 2]
#df4 = df[df.cluster == 3]
centroid = df.groupby('cluster').mean()[['Income','NumWebPurchases']]

plt.scatter(df1['Income'],df1['NumWebPurchases'],color="blue")
plt.scatter(df2['Income'],df2['NumWebPurchases'],color="black")
plt.scatter(df3['Income'],df3['NumWebPurchases'],color="green")
#plt.scatter(df4['Income'],df4['NumWebPurchases'],color="purple")

plt.scatter(centroid['Income'],centroid['NumWebPurchases'],marker='*',color="red")

k_rng = range(1,10)
sse = []
for k in k_rng:
  km = KMeans(n_clusters=k)
  km.fit_predict(df)
  sse.append(km.inertia_)

plt.xlabel('Clusters')
plt.ylabel('Sum of squared error')
plt.plot(k_rng,sse,marker='.')

print(silhouette_score(df[['ID','NumWebPurchases']],y_pred))

print("Inertia:", km.inertia_)

